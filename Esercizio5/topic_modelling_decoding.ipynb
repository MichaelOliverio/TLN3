{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import nltk\n",
    "from nltk import MWETokenizer, WordNetLemmatizer\n",
    "from nltk.corpus import wordnet as wn, stopwords\n",
    "import gensim\n",
    "from gensim import corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english')) \n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocessing(text):\n",
    "    text = re.sub(r'[^\\w\\s]',' ',text) # rimuovo la punteggiatura\n",
    "    text = text.lower()\n",
    "    text = nltk.pos_tag(text.split()) # prendo i pos tag delle parole (fa anche il tokenizing)\n",
    "    text = [x for x in text if x[1] in ['NN','NNS','NNP','NNPS']] # mantengo solo i noun\n",
    "    text = [x[0] for x in text] # rimuovo i pos tag\n",
    "    text = [lemmatizer.lemmatize(x) for x in text]\n",
    "    text = [x for x in text if x not in stop_words] # rimuovo le stop words\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = [\n",
    "    \"documents\\\\bbc\\\\entertainment\", \n",
    "    \"documents\\\\bbcsport\\\\athletics\", \n",
    "    \"documents\\\\bbcsport\\\\cricket\", \n",
    "    \"documents\\\\bbcsport\\\\football\", \n",
    "    \"documents\\\\bbcsport\\\\rugby\",\n",
    "    \"documents\\\\bbcsport\\\\tennis\"\n",
    "]\n",
    "\n",
    "documents = []\n",
    "\n",
    "for path in paths:\n",
    "    for file_name in os.listdir(path):\n",
    "        if os.path.isfile(os.path.join(path, file_name)):\n",
    "            file = open(path + \"/\" + file_name, \"r\", encoding=\"utf-8\")\n",
    "            document = preprocessing(file.read())\n",
    "            documents.append(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numero di documenti di training: 779\n",
      "Numero di documenti di test: 86\n",
      "Numero di documenti totali: 865\n"
     ]
    }
   ],
   "source": [
    "# crea un array test_documents con il 10% dei documenti e rimuove i documenti di test da documents\n",
    "test_documents = []\n",
    "training_documents = documents.copy()\n",
    "\n",
    "for i in range(0, int(len(documents) * 0.1)):\n",
    "    random_index = np.random.randint(0, len(training_documents)-1)\n",
    "    test_documents.append(training_documents[random_index])\n",
    "    training_documents.pop(random_index)\n",
    "\n",
    "print(\"Numero di documenti di training: \" + str(len(training_documents)))\n",
    "print(\"Numero di documenti di test: \" + str(len(test_documents)))\n",
    "print(\"Numero di documenti totali: \" + str(len(documents)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: 0.019*\"england\" + 0.016*\"test\" + 0.010*\"club\" + 0.007*\"woman\" + 0.006*\"kenteris\" + 0.006*\"week\" + 0.006*\"sport\" + 0.006*\"iaaf\" + 0.006*\"month\" + 0.006*\"chelsea\"\n",
      "Topic 1: 0.012*\"film\" + 0.010*\"club\" + 0.010*\"manager\" + 0.007*\"man\" + 0.007*\"cup\" + 0.007*\"minute\" + 0.006*\"half\" + 0.006*\"rugby\" + 0.005*\"star\" + 0.005*\"france\"\n",
      "Topic 2: 0.012*\"club\" + 0.012*\"cup\" + 0.011*\"football\" + 0.011*\"number\" + 0.008*\"people\" + 0.007*\"action\" + 0.007*\"month\" + 0.007*\"bos\" + 0.006*\"thing\" + 0.006*\"jones\"\n",
      "Topic 3: 0.009*\"jones\" + 0.008*\"minute\" + 0.007*\"way\" + 0.007*\"test\" + 0.007*\"break\" + 0.006*\"cricket\" + 0.006*\"england\" + 0.006*\"point\" + 0.006*\"win\" + 0.006*\"wale\"\n",
      "Topic 4: 0.010*\"minute\" + 0.008*\"ball\" + 0.008*\"england\" + 0.008*\"chance\" + 0.007*\"champion\" + 0.007*\"goal\" + 0.006*\"home\" + 0.005*\"liverpool\" + 0.005*\"film\" + 0.005*\"point\"\n",
      "Topic 5: 0.023*\"england\" + 0.010*\"injury\" + 0.010*\"wale\" + 0.009*\"goal\" + 0.009*\"nation\" + 0.008*\"robinson\" + 0.007*\"club\" + 0.007*\"music\" + 0.007*\"cup\" + 0.006*\"ball\"\n",
      "Topic 6: 0.014*\"test\" + 0.012*\"ireland\" + 0.012*\"england\" + 0.010*\"france\" + 0.008*\"title\" + 0.008*\"cricket\" + 0.008*\"number\" + 0.007*\"ball\" + 0.007*\"coach\" + 0.007*\"minute\"\n",
      "Topic 7: 0.014*\"ball\" + 0.011*\"goal\" + 0.010*\"england\" + 0.009*\"city\" + 0.008*\"season\" + 0.007*\"way\" + 0.007*\"minute\" + 0.006*\"award\" + 0.006*\"club\" + 0.006*\"rugby\"\n",
      "Topic 8: 0.011*\"test\" + 0.009*\"england\" + 0.008*\"wicket\" + 0.007*\"tour\" + 0.007*\"music\" + 0.006*\"win\" + 0.006*\"wale\" + 0.006*\"ball\" + 0.006*\"zealand\" + 0.006*\"rugby\"\n",
      "Topic 9: 0.009*\"week\" + 0.008*\"champion\" + 0.008*\"coach\" + 0.008*\"test\" + 0.007*\"injury\" + 0.007*\"zealand\" + 0.007*\"goal\" + 0.007*\"ball\" + 0.006*\"chelsea\" + 0.006*\"run\"\n",
      "Topic 10: 0.010*\"season\" + 0.009*\"england\" + 0.009*\"ball\" + 0.008*\"film\" + 0.008*\"chelsea\" + 0.007*\"week\" + 0.006*\"champion\" + 0.006*\"win\" + 0.006*\"goal\" + 0.006*\"robinson\"\n",
      "Topic 11: 0.016*\"v\" + 0.009*\"england\" + 0.008*\"ireland\" + 0.007*\"rugby\" + 0.007*\"club\" + 0.006*\"deal\" + 0.006*\"season\" + 0.005*\"people\" + 0.005*\"star\" + 0.005*\"record\"\n",
      "Topic 12: 0.021*\"film\" + 0.008*\"ball\" + 0.007*\"series\" + 0.007*\"injury\" + 0.006*\"test\" + 0.006*\"award\" + 0.006*\"victory\" + 0.006*\"win\" + 0.006*\"director\" + 0.005*\"minute\"\n",
      "Topic 13: 0.012*\"season\" + 0.010*\"club\" + 0.009*\"cup\" + 0.009*\"champion\" + 0.008*\"liverpool\" + 0.007*\"england\" + 0.007*\"victory\" + 0.006*\"week\" + 0.006*\"manager\" + 0.006*\"france\"\n",
      "Topic 14: 0.011*\"series\" + 0.010*\"film\" + 0.009*\"cricket\" + 0.008*\"home\" + 0.007*\"goal\" + 0.006*\"england\" + 0.006*\"minute\" + 0.006*\"month\" + 0.005*\"country\" + 0.005*\"test\"\n"
     ]
    }
   ],
   "source": [
    "# Creo il dizionario\n",
    "dictionary = corpora.Dictionary(training_documents)\n",
    "dictionary.filter_extremes(no_below=5, no_above=0.3, keep_n=None)  # use Dictionary to remove un-relevant tokens\n",
    "\n",
    "# Creo la rappresentazione del corpus\n",
    "corpus = [dictionary.doc2bow(doc) for doc in training_documents]\n",
    "\n",
    "# Definisco il modello LDA\n",
    "k = 15  # Numero di topic da identificare\n",
    "lda_model = gensim.models.LdaModel(corpus, num_topics=k, id2word=dictionary)\n",
    "\n",
    "# Visualizzazione dei topic identificati\n",
    "for topic_id, topic in lda_model.show_topics(formatted=True, num_topics=k, num_words=10):\n",
    "    print(f\"Topic {topic_id}: {topic}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic distribution for new document:\n",
      "Topic 0: 0.08125948160886765\n",
      "Topic 1: 0.04164519160985947\n",
      "Topic 4: 0.05920872837305069\n",
      "Topic 5: 0.048244908452034\n",
      "Topic 6: 0.01747971773147583\n",
      "Topic 7: 0.02704409882426262\n",
      "Topic 8: 0.024588219821453094\n",
      "Topic 9: 0.03239960968494415\n",
      "Topic 10: 0.03977017104625702\n",
      "Topic 12: 0.045488741248846054\n",
      "Topic 13: 0.3618888258934021\n",
      "Topic 14: 0.21740493178367615\n"
     ]
    }
   ],
   "source": [
    "# Inferenza dei topic per un nuovo documento\n",
    "flattened_test_documents = [token for sublist in test_documents for token in sublist]\n",
    "new_bow = dictionary.doc2bow(flattened_test_documents)\n",
    "topic_distribution = lda_model.get_document_topics(new_bow)\n",
    "\n",
    "print(\"Topic distribution for new document:\")\n",
    "for topic_id, topic_prob in topic_distribution:\n",
    "    print(f\"Topic {topic_id}: {topic_prob}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
