{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Laboratorio 5\n",
    "\n",
    "Si richiede un'implementazione di un esercizio di Topic Modeling, utilizzando librerie open (come ad es. GenSim (https://radimrehurek.com/gensim/). Si richiede l'utilizzo di un corpus di almeno 1k documenti. Testare un algoritmo (ad esempio LDA) con più valori di k (num. di topics) e valutare la coerenza dei risultati, attraverso fine-tuning su parametri e pre-processing. Update: essendo che spesso i topic, per essere interpretabili, devono contenere content words, potete pensare di filtrare solamente i sostantivi in fase di preprocessing (cioè POS=noun)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import nltk\n",
    "from nltk import MWETokenizer, WordNetLemmatizer\n",
    "from nltk.corpus import wordnet as wn, stopwords\n",
    "import gensim\n",
    "from gensim import corpora"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing delle frasi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['plain', 'spruce', 'gallery', 'bulb', 'adornment', 'one', 'plate', 'branch', 'child', 'charity', 'artwork', 'assistant', 'henry', 'moore', '1960s', 'reputation', 'sculptor', 'teacher', 'decade', 'wentworth', 'photography', 'mundane', 'subject', 'cigarette', 'packet', 'leg', 'table']\n"
     ]
    }
   ],
   "source": [
    "stop_words = set(stopwords.words('english')) \n",
    "mwes = [x for x in wn.all_lemma_names() if '_' in x]\n",
    "mwes = [tuple(x.split('_')) for x in mwes]\n",
    "#tokenizer = MWETokenizer(mwes, separator=' ')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocessing(text):\n",
    "    text = re.sub(r'[^\\w\\s]',' ',text) # rimuovo la punteggiatura\n",
    "    text = text.lower()\n",
    "    text = nltk.pos_tag(text.split()) # prendo i pos tag delle parole (fa anche il tokenizing)\n",
    "    text = [x for x in text if x[1] in ['NN','NNS','NNP','NNPS']] # mantengo solo i noun\n",
    "    text = [x[0] for x in text] # rimuovo i pos tag\n",
    "    text = [lemmatizer.lemmatize(x) for x in text]\n",
    "    text = [x for x in text if x not in stop_words]\n",
    "    return text\n",
    "\n",
    "text = \"The plain green Norway spruce is displayed in the gallery's foyer. Its light bulb adornments are dimmed, ordinary domestic ones joined together with string. The plates decorating the branches will be auctioned off for the children's charity ArtWorks. Wentworth worked as an assistant to sculptor Henry Moore in the late 1960s. His reputation as a sculptor grew in the 1980s, while he has been one of the most influential teachers during the last two decades. Wentworth is also known for his photography of mundane, everyday subjects such as a cigarette packet jammed under the wonky leg of a table.\"\n",
    "text = preprocessing(text)\n",
    "print(text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prelevo i documenti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = [\n",
    "    \"documents\\\\bbc\\\\entertainment\", \n",
    "    \"documents\\\\bbcsport\\\\athletics\", \n",
    "    \"documents\\\\bbcsport\\\\cricket\", \n",
    "    \"documents\\\\bbcsport\\\\football\", \n",
    "    \"documents\\\\bbcsport\\\\rugby\",\n",
    "    \"documents\\\\bbcsport\\\\tennis\"\n",
    "]\n",
    "\n",
    "documents = []\n",
    "\n",
    "for path in paths:\n",
    "    for file_name in os.listdir(path):\n",
    "        if os.path.isfile(os.path.join(path, file_name)):\n",
    "            file = open(path + \"/\" + file_name, \"r\", encoding=\"utf-8\")\n",
    "            document = preprocessing(file.read())\n",
    "            documents.append(document)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Divido in training set e test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numero di documenti di training: 779\n",
      "Numero di documenti di test: 86\n",
      "Numero di documenti totali: 865\n"
     ]
    }
   ],
   "source": [
    "# crea un array test_documents con il 10% dei documenti e rimuove i documenti di test da documents\n",
    "test_documents = []\n",
    "training_documents = documents.copy()\n",
    "\n",
    "for i in range(0, int(len(documents) * 0.1)):\n",
    "    random_index = np.random.randint(0, len(training_documents)-1)\n",
    "    test_documents.append(training_documents[random_index])\n",
    "    training_documents.pop(random_index)\n",
    "\n",
    "print(\"Numero di documenti di training: \" + str(len(training_documents)))\n",
    "print(\"Numero di documenti di test: \" + str(len(test_documents)))\n",
    "print(\"Numero di documenti totali: \" + str(len(documents)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: 0.013*\"minute\" + 0.009*\"england\" + 0.008*\"victory\" + 0.008*\"nation\" + 0.008*\"goal\" + 0.007*\"club\" + 0.007*\"ireland\" + 0.006*\"way\" + 0.006*\"wale\" + 0.006*\"cup\"\n",
      "Topic 1: 0.013*\"england\" + 0.009*\"week\" + 0.009*\"injury\" + 0.008*\"series\" + 0.007*\"cricket\" + 0.007*\"season\" + 0.007*\"coach\" + 0.007*\"number\" + 0.006*\"test\" + 0.006*\"tour\"\n",
      "Topic 2: 0.010*\"film\" + 0.009*\"cup\" + 0.006*\"england\" + 0.006*\"sport\" + 0.005*\"jones\" + 0.005*\"decision\" + 0.005*\"chance\" + 0.005*\"football\" + 0.005*\"ball\" + 0.005*\"chelsea\"\n",
      "Topic 3: 0.012*\"number\" + 0.012*\"film\" + 0.011*\"race\" + 0.010*\"award\" + 0.008*\"week\" + 0.006*\"cup\" + 0.005*\"goal\" + 0.005*\"place\" + 0.005*\"half\" + 0.005*\"home\"\n",
      "Topic 4: 0.014*\"club\" + 0.012*\"cup\" + 0.010*\"chelsea\" + 0.008*\"football\" + 0.008*\"goal\" + 0.007*\"ball\" + 0.006*\"v\" + 0.006*\"manager\" + 0.005*\"season\" + 0.005*\"test\"\n",
      "Topic 5: 0.010*\"champion\" + 0.009*\"england\" + 0.009*\"series\" + 0.007*\"test\" + 0.007*\"season\" + 0.006*\"wale\" + 0.006*\"club\" + 0.006*\"jones\" + 0.005*\"bbc\" + 0.005*\"country\"\n",
      "Topic 6: 0.015*\"test\" + 0.011*\"club\" + 0.008*\"cricket\" + 0.008*\"film\" + 0.006*\"champion\" + 0.006*\"england\" + 0.006*\"home\" + 0.005*\"france\" + 0.005*\"series\" + 0.005*\"ireland\"\n",
      "Topic 7: 0.011*\"injury\" + 0.008*\"england\" + 0.007*\"win\" + 0.007*\"woman\" + 0.007*\"test\" + 0.006*\"ball\" + 0.006*\"cup\" + 0.005*\"club\" + 0.005*\"round\" + 0.005*\"goal\"\n",
      "Topic 8: 0.014*\"ball\" + 0.008*\"champion\" + 0.008*\"england\" + 0.006*\"minute\" + 0.006*\"run\" + 0.006*\"test\" + 0.006*\"goal\" + 0.005*\"injury\" + 0.005*\"star\" + 0.005*\"chelsea\"\n",
      "Topic 9: 0.017*\"film\" + 0.011*\"england\" + 0.008*\"chelsea\" + 0.007*\"wale\" + 0.006*\"half\" + 0.005*\"chance\" + 0.005*\"goal\" + 0.005*\"season\" + 0.005*\"league\" + 0.005*\"test\"\n"
     ]
    }
   ],
   "source": [
    "# Creo il dizionario\n",
    "dictionary = corpora.Dictionary(training_documents)\n",
    "dictionary.filter_extremes(no_below=5, no_above=0.3, keep_n=None)  # use Dictionary to remove un-relevant tokens\n",
    "\n",
    "# Creo la rappresentazione del corpus\n",
    "corpus = [dictionary.doc2bow(doc) for doc in training_documents]\n",
    "\n",
    "# Definisco il modello LDA\n",
    "k = 10  # Numero di topic da identificare\n",
    "lda_model = gensim.models.LdaModel(corpus, num_topics=k, id2word=dictionary)\n",
    "\n",
    "# Visualizzazione dei topic identificati\n",
    "for topic_id, topic in lda_model.show_topics(formatted=True, num_topics=k, num_words=10):\n",
    "    print(f\"Topic {topic_id}: {topic}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic distribution for new document:\n",
      "Topic 0: 0.3191267251968384\n",
      "Topic 1: 0.2813403308391571\n",
      "Topic 2: 0.05434315279126167\n",
      "Topic 3: 0.05442822724580765\n",
      "Topic 4: 0.05994976684451103\n",
      "Topic 6: 0.09403547644615173\n",
      "Topic 7: 0.010968337766826153\n",
      "Topic 8: 0.08119918406009674\n",
      "Topic 9: 0.040543898940086365\n"
     ]
    }
   ],
   "source": [
    "# Inferenza dei topic per un nuovo documento\n",
    "flattened_test_documents = [token for sublist in test_documents for token in sublist]\n",
    "new_bow = dictionary.doc2bow(flattened_test_documents)\n",
    "topic_distribution = lda_model.get_document_topics(new_bow)\n",
    "\n",
    "print(\"Topic distribution for new document:\")\n",
    "for topic_id, topic_prob in topic_distribution:\n",
    "    print(f\"Topic {topic_id}: {topic_prob}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
