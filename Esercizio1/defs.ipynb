{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import string\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "corpus = pd.read_csv('definizioni.csv', sep=',', engine='python')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Per ogni concetto, prendere tutte le definizioni, e calcolare la sovrapposizione lessicale tra tutte le definizioni (cioè le parole in comune). E' possibile che per un dato concetto l'intersezione delle parole sia vuota. Ad esempio, se una definizione contiene una parola che non viene usata dalle altre non verrà conteggiata. Un modo è contare le parole più ricorrenti (contando la loro frequenza) e poi vedere quanto quelle parole ci sono nelle definizioni (es. 12/30 usano una certa parola) e da qui calcoliamo uno score delle parole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_sentence(sentence, lemmatizer = None, stemmer = None):\n",
    "    \"\"\"\n",
    "    Data una frase la pulisce dai caratteri speciali e lettere maiscole\n",
    "    e ed esegue infine la lemmatizzazione o stemmatizzazione, in base ai\n",
    "    parametri specificati. Oltre a ciò viene fatto lo stopword removal, cioè\n",
    "    vengono tolte parole congiunzioni, articoli, ...\n",
    "    Ritorna un array di parole.\n",
    "    \"\"\"\n",
    "    from nltk.corpus import stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    if lemmatizer is None and stemmer is None:\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    tokens = nltk.word_tokenize(sentence)\n",
    "    tokens = [token for token in tokens if token not in string.punctuation] #tolgo la punteggiatura\n",
    "    tokens = [token.lower() for token in tokens] # sostituisco le maiuscole con le minuscole\n",
    "    tokens = [token for token in tokens if token not in stop_words] # rimuovo le stop words\n",
    "    if stemmer is not None:\n",
    "        tokens = [stemmer.stem(token) for token in tokens] # stemmizzo\n",
    "    else:\n",
    "        tokens = [lemmatizer.lemmatize(token) for token in tokens] # lemmatizzo\n",
    "    \n",
    "    return tokens\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokens_frequency(sentences):\n",
    "    \"\"\"\n",
    "    Data una lista di frasi conta le occorrenze di ogni parola e ritorna un dizionario\n",
    "    con la loro frequenza ordinato dalla parola più frequente a quella meno.\n",
    "    \"\"\"\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    stemmer = PorterStemmer() # non usato\n",
    "\n",
    "    sentences_number = len(sentences)\n",
    "    tokens_occurrences = {}\n",
    "    for i in range (1, len(sentences)):\n",
    "        sentence = sentences[i]\n",
    "\n",
    "        if not isinstance(sentence, float):\n",
    "            #tokens = clear_sentence(sentence, lemmatizer, None)\n",
    "            tokens = clear_sentence(sentence, None, stemmer)\n",
    "\n",
    "            for token in tokens:\n",
    "                if token in tokens_occurrences:\n",
    "                    tokens_occurrences[token] += 1\n",
    "                else:\n",
    "                    tokens_occurrences[token] = 1\n",
    "    \n",
    "    tokens_frequency = {}\n",
    "\n",
    "    for token in tokens_occurrences:\n",
    "        tokens_frequency[token] = tokens_occurrences[token] / sentences_number\n",
    "\n",
    "    # riordino le frequenze in ordine decrescente\n",
    "    tokens_frequency = {k: v for k, v in sorted(tokens_frequency.items(), key=lambda item: item[1], reverse=True)}\n",
    "\n",
    "    return tokens_frequency\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.09762794384057971\n",
      "0.09722793311403509\n",
      "0.179736328125\n",
      "0.3218845274390244\n"
     ]
    }
   ],
   "source": [
    "#emotion definition\n",
    "emotion_definitions = corpus.iloc[0]\n",
    "emotion_definitions = emotion_definitions[1:] #remove the first column because it is the name of the emotion\n",
    "tokens_frequency_for_emotion_definition = get_tokens_frequency(emotion_definitions)\n",
    "#calculate an average of the frequency of the words\n",
    "average_frequency = 0\n",
    "occurrences = 0\n",
    "for token in tokens_frequency_for_emotion_definition:\n",
    "    #per ogni emtion defeinition\n",
    "    for sentence in emotion_definitions:\n",
    "        if not isinstance(sentence, float):\n",
    "            if token in sentence:\n",
    "                occurrences += 1\n",
    "    average_frequency += (occurrences / len(emotion_definitions)) * tokens_frequency_for_emotion_definition[token]\n",
    "average_frequency = average_frequency / len(tokens_frequency_for_emotion_definition)\n",
    "print(average_frequency)\n",
    "\n",
    "#person definition\n",
    "person_definitions = corpus.iloc[1]\n",
    "person_definitions = person_definitions[1:] #remove the first column because it is the name of the person\n",
    "tokens_frequency_for_person_definition = get_tokens_frequency(person_definitions)\n",
    "#calculate an average of the frequency of the words\n",
    "average_frequency = 0\n",
    "occurrences = 0\n",
    "for token in tokens_frequency_for_person_definition:\n",
    "    #per ogni emtion defeinition\n",
    "    for sentence in person_definitions:\n",
    "        if not isinstance(sentence, float):\n",
    "            if token in sentence:\n",
    "                occurrences += 1\n",
    "    average_frequency += (occurrences / len(person_definitions)) * tokens_frequency_for_person_definition[token]\n",
    "average_frequency = average_frequency / len(tokens_frequency_for_person_definition)\n",
    "print(average_frequency)\n",
    "\n",
    "#revenge definition\n",
    "revenge_definitions = corpus.iloc[2]\n",
    "revenge_definitions = revenge_definitions[1:] #remove the first column because it is the name of the revenge\n",
    "tokens_frequency_for_revenge_definition = get_tokens_frequency(revenge_definitions)\n",
    "#calculate an average of the frequency of the words\n",
    "average_frequency = 0\n",
    "occurrences = 0\n",
    "for token in tokens_frequency_for_revenge_definition:\n",
    "    #per ogni emtion defeinition\n",
    "    for sentence in revenge_definitions:\n",
    "        if not isinstance(sentence, float):\n",
    "            if token in sentence:\n",
    "                occurrences += 1\n",
    "    average_frequency += (occurrences / len(revenge_definitions)) * tokens_frequency_for_revenge_definition[token]\n",
    "average_frequency = average_frequency / len(tokens_frequency_for_revenge_definition)\n",
    "print(average_frequency)\n",
    "\n",
    "#brick definition\n",
    "brick_definitions = corpus.iloc[3]\n",
    "brick_definitions = brick_definitions[1:] #remove the first column because it is the name of the brick\n",
    "tokens_frequency_for_brick_definition = get_tokens_frequency(brick_definitions)\n",
    "#calculate an average of the frequency of the words\n",
    "average_frequency = 0\n",
    "occurrences = 0\n",
    "for token in tokens_frequency_for_brick_definition:\n",
    "    #per ogni emtion defeinition\n",
    "    for sentence in brick_definitions:\n",
    "        if not isinstance(sentence, float):\n",
    "            if token in sentence:\n",
    "                occurrences += 1\n",
    "    average_frequency += (occurrences / len(brick_definitions)) * tokens_frequency_for_brick_definition[token]\n",
    "average_frequency = average_frequency / len(tokens_frequency_for_brick_definition)\n",
    "print(average_frequency)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DA FARLO PER LE ALTRE PAROLE E POI AGGREGARE"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una volta definita la metrica per la sovrapposizione lessicale, aggreghiamo le dimensioni proposte. Cioè, prendiamo i due concetti concreti e vediamo com'è l'andamento delle parole, e idem per quelli astratti. Faremo la stessa cosa per quelli generici e quelli specifici.\n",
    "\n",
    "Ciò viene fatto per vedere se ci sono differenze tra le dimensioni. Tipicamente, c'è molta più sovrapposizione su concetti concreti e su concetti specifici"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Person and brick definition\n",
      "           Frequency\n",
      "human       0.437500\n",
      "use         0.375000\n",
      "build       0.359375\n",
      "construct   0.281250\n",
      "object      0.250000\n",
      "materi      0.250000\n",
      "person      0.093750\n",
      "made        0.093750\n",
      "clay        0.093750\n",
      "block       0.093750\n",
      "0.09606684470663265\n",
      "\n",
      "Emotion and revenge definition\n",
      "          Frequency\n",
      "feel       0.406250\n",
      "someon     0.234375\n",
      "someth     0.203125\n",
      "emot       0.140625\n",
      "anger      0.125000\n",
      "human      0.109375\n",
      "action     0.109375\n",
      "reaction   0.109375\n",
      "act        0.093750\n",
      "hurt       0.093750\n",
      "0.08717805195630081\n",
      "\n",
      "Person and emotion definition\n",
      "         Frequency\n",
      "human     0.562500\n",
      "feel      0.328125\n",
      "live      0.125000\n",
      "person    0.109375\n",
      "someth    0.109375\n",
      "certain   0.078125\n",
      "anim      0.062500\n",
      "state     0.062500\n",
      "mental    0.062500\n",
      "entiti    0.046875\n",
      "0.06491142406798246\n",
      "\n",
      "Brick and revenge definition\n",
      "           Frequency\n",
      "use         0.375000\n",
      "build       0.343750\n",
      "construct   0.265625\n",
      "materi      0.234375\n",
      "object      0.234375\n",
      "someon      0.218750\n",
      "someth      0.171875\n",
      "anger       0.125000\n",
      "feel        0.109375\n",
      "emot        0.109375\n",
      "0.12014180672268908\n"
     ]
    }
   ],
   "source": [
    "#person and brick definition (aggregazione di concetti concreti)\n",
    "person_definitions = corpus.iloc[1]\n",
    "brick_definitions = corpus.iloc[3]\n",
    "person_definitions = person_definitions[1:] #remove the first column because it is the name of the person\n",
    "brick_definitions = brick_definitions[1:] #remove the first column because it is the name of the brick\n",
    "person_and_brick_definitions = pd.concat([person_definitions, brick_definitions], axis=0)\n",
    "tokens_frequency_for_person_and_brick_definition = get_tokens_frequency(person_and_brick_definitions)\n",
    "#show in a table the 10 most frequent words\n",
    "table = pd.DataFrame.from_dict(tokens_frequency_for_person_and_brick_definition, orient='index').head(10)\n",
    "table.columns = ['Frequency']\n",
    "print(\"Person and brick definition\")\n",
    "print(table)\n",
    "#calculate an average of the frequency of the words\n",
    "average_frequency = 0\n",
    "occurrences = 0\n",
    "for token in tokens_frequency_for_person_and_brick_definition:\n",
    "    #per ogni emtion defeinition\n",
    "    for sentence in person_and_brick_definitions:\n",
    "        if not isinstance(sentence, float):\n",
    "            if token in sentence:\n",
    "                occurrences += 1\n",
    "    average_frequency += (occurrences / len(person_and_brick_definitions)) * tokens_frequency_for_person_and_brick_definition[token]\n",
    "average_frequency = average_frequency / len(tokens_frequency_for_person_and_brick_definition)\n",
    "print(average_frequency)\n",
    "\n",
    "#emotion and revenge definition (aggregazioni di concetti astraggi)\n",
    "emotion_definitions = corpus.iloc[0]\n",
    "revenge_definitions = corpus.iloc[2]\n",
    "emotion_definitions = emotion_definitions[1:] #remove the first column because it is the name of the emotion\n",
    "revenge_definitions = revenge_definitions[1:] #remove the first column because it is the name of the revenge\n",
    "emotion_and_revenge_definitions = pd.concat([emotion_definitions, revenge_definitions], axis=0)\n",
    "tokens_frequency_for_emotion_and_revenge_definition = get_tokens_frequency(emotion_and_revenge_definitions)\n",
    "#show in a table the 10 most frequent words\n",
    "table = pd.DataFrame.from_dict(tokens_frequency_for_emotion_and_revenge_definition, orient='index').head(10)\n",
    "table.columns = ['Frequency']\n",
    "print(\"\\nEmotion and revenge definition\")\n",
    "print(table)\n",
    "#calculate an average of the frequency of the words\n",
    "average_frequency = 0\n",
    "occurrences = 0\n",
    "for token in tokens_frequency_for_emotion_and_revenge_definition:\n",
    "    #per ogni emtion defeinition\n",
    "    for sentence in emotion_and_revenge_definitions:\n",
    "        if not isinstance(sentence, float):\n",
    "            if token in sentence:\n",
    "                occurrences += 1\n",
    "    average_frequency += (occurrences / len(emotion_and_revenge_definitions)) * tokens_frequency_for_emotion_and_revenge_definition[token]\n",
    "average_frequency = average_frequency / len(tokens_frequency_for_emotion_and_revenge_definition)\n",
    "print(average_frequency)\n",
    "\n",
    "#person and emotion definition (aggragazione di concetti generici)\n",
    "person_definitions = corpus.iloc[1]\n",
    "emotion_definitions = corpus.iloc[0]\n",
    "person_definitions = person_definitions[1:] #remove the first column because it is the name of the person\n",
    "emotion_definitions = emotion_definitions[1:] #remove the first column because it is the name of the emotion\n",
    "person_and_emotion_definitions = pd.concat([person_definitions, emotion_definitions], axis=0)\n",
    "tokens_frequency_for_person_and_emotion_definition = get_tokens_frequency(person_and_emotion_definitions)\n",
    "#show in a table the 10 most frequent words\n",
    "table = pd.DataFrame.from_dict(tokens_frequency_for_person_and_emotion_definition, orient='index').head(10)\n",
    "table.columns = ['Frequency']\n",
    "print(\"\\nPerson and emotion definition\")\n",
    "print(table)\n",
    "#calculate an average of the frequency of the words\n",
    "average_frequency = 0\n",
    "occurrences = 0\n",
    "for token in tokens_frequency_for_person_and_emotion_definition:\n",
    "    #per ogni emtion defeinition\n",
    "    for sentence in person_and_emotion_definitions:\n",
    "        if not isinstance(sentence, float):\n",
    "            if token in sentence:\n",
    "                occurrences += 1\n",
    "    average_frequency += (occurrences / len(person_and_emotion_definitions)) * tokens_frequency_for_person_and_emotion_definition[token]\n",
    "average_frequency = average_frequency / len(tokens_frequency_for_person_and_emotion_definition)\n",
    "print(average_frequency)\n",
    "\n",
    "#brick and revenge definition (aggragazione di concetti specifici)\n",
    "brick_definitions = corpus.iloc[3]\n",
    "revenge_definitions = corpus.iloc[2]\n",
    "brick_definitions = brick_definitions[1:] #remove the first column because it is the name of the brick\n",
    "revenge_definitions = revenge_definitions[1:] #remove the first column because it is the name of the revenge\n",
    "brick_and_revenge_definitions = pd.concat([brick_definitions, revenge_definitions], axis=0)\n",
    "tokens_frequency_for_brick_and_revenge_definition = get_tokens_frequency(brick_and_revenge_definitions)\n",
    "#show in a table the 10 most frequent words\n",
    "table = pd.DataFrame.from_dict(tokens_frequency_for_brick_and_revenge_definition, orient='index').head(10)\n",
    "table.columns = ['Frequency']\n",
    "print(\"\\nBrick and revenge definition\")\n",
    "print(table)\n",
    "#calculate an average of the frequency of the words\n",
    "average_frequency = 0\n",
    "occurrences = 0\n",
    "for token in tokens_frequency_for_brick_and_revenge_definition:\n",
    "    #per ogni emtion defeinition\n",
    "    for sentence in brick_and_revenge_definitions:\n",
    "        if not isinstance(sentence, float):\n",
    "            if token in sentence:\n",
    "                occurrences += 1\n",
    "    average_frequency += (occurrences / len(brick_and_revenge_definitions)) * tokens_frequency_for_brick_and_revenge_definition[token]\n",
    "average_frequency = average_frequency / len(tokens_frequency_for_brick_and_revenge_definition)\n",
    "print(average_frequency)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0 (main, Oct 24 2022, 18:26:48) [MSC v.1933 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5238573367df39f7286bb46f9ff5f08f63a01a80960060ce41e3c79b190280fa"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
