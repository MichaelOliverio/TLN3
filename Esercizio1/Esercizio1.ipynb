{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "corpus = pd.read_csv('definizioni.csv', sep=',', engine='python')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Per ogni concetto, prendere tutte le definizioni, e calcolare la sovrapposizione lessicale tra tutte le definizioni (cioè le parole in comune). E' possibile che per un dato concetto l'intersezione delle parole sia vuota. Ad esempio, se una definizione contiene una parola che non viene usata dalle altre non verrà conteggiata. Un modo è contare le parole più ricorrenti (contando la loro frequenza) e poi vedere quanto quelle parole ci sono nelle definizioni (es. 12/30 usano una certa parola) e da qui calcoliamo uno score delle parole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'human': 0.90625,\n",
       " 'being': 0.71875,\n",
       " 'a': 0.53125,\n",
       " 'is': 0.25,\n",
       " 'to': 0.21875,\n",
       " 'person': 0.1875,\n",
       " 'the': 0.15625,\n",
       " 'living': 0.125,\n",
       " 'or': 0.125,\n",
       " 'and': 0.125,\n",
       " 'it': 0.09375,\n",
       " 'with': 0.09375,\n",
       " 'individual': 0.09375,\n",
       " 'that': 0.09375,\n",
       " 'certain': 0.09375,\n",
       " 'such': 0.09375,\n",
       " 'ability': 0.09375,\n",
       " 'for': 0.0625,\n",
       " 'single': 0.0625,\n",
       " 'some': 0.0625,\n",
       " 'from': 0.0625,\n",
       " 'entity': 0.0625,\n",
       " 'can': 0.0625,\n",
       " 'characteristic': 0.0625,\n",
       " 'of': 0.0625,\n",
       " 'homo': 0.0625,\n",
       " 'sapiens': 0.0625,\n",
       " 'answer': 0.0625,\n",
       " 'question': 0.0625,\n",
       " 'what': 0.0625,\n",
       " 'mean': 0.0625,\n",
       " 'be': 0.0625,\n",
       " 'may': 0.0625,\n",
       " 'say': 0.0625,\n",
       " 'by': 0.0625,\n",
       " 'generic': 0.03125,\n",
       " 'describe': 0.03125,\n",
       " 'precise': 0.03125,\n",
       " 'feature': 0.03125,\n",
       " 'belonging': 0.03125,\n",
       " 'group': 0.03125,\n",
       " 'society': 0.03125,\n",
       " 'mammal': 0.03125,\n",
       " 'descending': 0.03125,\n",
       " 'ape': 0.03125,\n",
       " 'sentient': 0.03125,\n",
       " 'u': 0.03125,\n",
       " 'see': 0.03125,\n",
       " 'touch': 0.03125,\n",
       " 'all': 0.03125,\n",
       " 'his': 0.03125,\n",
       " 'animal': 0.03125,\n",
       " 'feeling': 0.03125,\n",
       " 'member': 0.03125,\n",
       " 'an': 0.03125,\n",
       " 'specie': 0.03125,\n",
       " 'man': 0.03125,\n",
       " 'woman': 0.03125,\n",
       " 'specific': 0.03125,\n",
       " 'there': 0.03125,\n",
       " 'no': 0.03125,\n",
       " 'this': 0.03125,\n",
       " 'vary': 0.03125,\n",
       " 'defined': 0.03125,\n",
       " 'physical': 0.03125,\n",
       " 'walk': 0.03125,\n",
       " 'upright': 0.03125,\n",
       " 'on': 0.03125,\n",
       " 'two': 0.03125,\n",
       " 'leg': 0.03125,\n",
       " 'cognitive': 0.03125,\n",
       " 'think': 0.03125,\n",
       " 'reason': 0.03125,\n",
       " 'others': 0.03125,\n",
       " 'about': 0.03125,\n",
       " 'having': 0.03125,\n",
       " 'emotion': 0.03125,\n",
       " 'experience': 0.03125,\n",
       " 'love': 0.03125,\n",
       " 'compassion': 0.03125,\n",
       " 'happiness': 0.03125,\n",
       " 'ultimately': 0.03125,\n",
       " 'complex': 0.03125,\n",
       " 'unique': 0.03125,\n",
       " 'each': 0.03125,\n",
       " 'must': 0.03125,\n",
       " 'themselves': 0.03125}"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import string\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "person_definitions = corpus.iloc[1]\n",
    "\n",
    "definitions_number = len(person_definitions) - 1 # -1 because the first column is the name of the person \n",
    "\n",
    "tokens_occurrences = {}\n",
    "\n",
    "for i in range (1, len(person_definitions)):\n",
    "    definition = person_definitions[i]\n",
    "\n",
    "    if not isinstance(definition, float):\n",
    "        tokens = nltk.word_tokenize(definition)\n",
    "        tokens = [token for token in tokens if token not in string.punctuation] #tolgo la punteggiatura\n",
    "        tokens = [token.lower() for token in tokens] # sostituisco le maiuscole con le minuscole\n",
    "        tokens = [lemmatizer.lemmatize(token) for token in tokens] # lemmatizzo\n",
    "\n",
    "        for token in tokens:\n",
    "            if token in tokens_occurrences:\n",
    "                tokens_occurrences[token] += 1\n",
    "            else:\n",
    "                tokens_occurrences[token] = 1\n",
    "   \n",
    "tokens_frequency = {}\n",
    "\n",
    "for word in tokens_occurrences:\n",
    "    tokens_frequency[word] = tokens_occurrences[word] / definitions_number\n",
    "\n",
    "tokens_frequency = {k: v for k, v in sorted(tokens_frequency.items(), key=lambda item: item[1], reverse=True)}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una volta definita la metrica per la sovrapposizione lessicale, aggreghiamo le dimensioni proposte. Cioè, prendiamo i due concetti concreti e vediamo com'è l'andamento delle parole, e idem per quelli astratti. Faremo la stessa cosa per quelli generici e quelli specifici.\n",
    "\n",
    "Ciò viene fatto per vedere se ci sono differenze tra le dimensioni. Tipicamente, c'è molta più sovrapposizione su concetti concreti e su concetti specifici"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5238573367df39f7286bb46f9ff5f08f63a01a80960060ce41e3c79b190280fa"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
