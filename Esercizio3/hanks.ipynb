{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Laboratorio 3\n",
    "\n",
    "Si richiede un’implementazione della teoria sulle valenze di Patrick Hanks. In particolare, partendo da un corpus a scelta e uno specifico verbo (tendenzialmente non troppo frequente e/o generico ma nemmeno raro), l’idea è di costruire dei possibili cluster semantici, con relativa frequenza. Ad es. dato il verbo \"to see\" con valenza = 2, e usando un parser sintattico (ad es. Spacy), si possono collezionare eventuali fillers per i ruoli di subj e obj del verbo, per poi convertirli in semantic types. Un cluster frequente su \"to see\" potrebbe unire subj = noun.person con obj = noun.artifact. Si richiede di partire da un corpus di almeno alcune centinaia di istanze del verbo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "import pandas as pd\n",
    "import string\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metodi di supporto"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalizzazione della frase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def normalize(sentence):\n",
    "    tokens = nltk.word_tokenize(sentence)\n",
    "    tokens = [token for token in tokens if token not in string.punctuation] #tolgo la punteggiatura\n",
    "    tokens = [token.lower() for token in tokens] # sostituisco le maiuscole con le minuscole\n",
    "    tokens = [token for token in tokens if token not in stop_words] # rimuovo le stop words\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens] # lemmatizzo\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algoritmo Simplified Lesk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simplified_lesk(word, context):\n",
    "    best_sense = None\n",
    "    max_overlap = 0\n",
    "\n",
    "    for sense in wn.synsets(word):\n",
    "        signature = set(normalize(sense.definition())).union(set(normalize(' '.join(sense.examples()))))\n",
    "        overlap = len(context.intersection(signature))\n",
    "        if overlap > max_overlap:\n",
    "            max_overlap = overlap\n",
    "            best_sense = sense\n",
    "    \n",
    "    return best_sense"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generazione delle strutture verbali con valenza 2 e slot subj e dobj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Metodo che per ogni verbo considera solo la valenza 2 andando a fare filling degli slot nsubj e dobj\n",
    "'''\n",
    "def generate(corpus):\n",
    "    parser = spacy.load('en_core_web_sm')\n",
    "    verb_structures = []\n",
    "    verb_clusters = []\n",
    "    phrase_clusters = []\n",
    "\n",
    "    for sentence in corpus['sentence']:\n",
    "        sentence = sentence.replace('<s>', '').replace('</s>', '').replace('\"', '').replace(\"'\", '')[1:]\n",
    "        parsified_sentence = parser(sentence)\n",
    "\n",
    "        for token in parsified_sentence:\n",
    "            if token.pos_ == 'VERB' and token.lemma_ == 'see':\n",
    "                nsubj = None\n",
    "                dobj = None\n",
    "                for child in token.children:\n",
    "                    if child.dep_ == 'nsubj':\n",
    "                        nsubj = child.text\n",
    "                    elif child.dep_ == 'dobj':\n",
    "                        dobj = child.text\n",
    "\n",
    "                if nsubj is not None and dobj is not None:\n",
    "                    synset_nsubj = simplified_lesk(nsubj, set(normalize(sentence)))\n",
    "                    synset_dobj = simplified_lesk(dobj, set(normalize(sentence)))\n",
    "\n",
    "                    if synset_nsubj is not None and synset_dobj is not None:\n",
    "                        semtype_nsubj = synset_nsubj.lexname().split('.')[1]\n",
    "                        semtype_dobj = synset_dobj.lexname().split('.')[1]\n",
    "\n",
    "                        verb_structures.append((nsubj, synset_nsubj, dobj, synset_dobj))\n",
    "                        verb_clusters.append((semtype_nsubj, semtype_dobj))\n",
    "                        phrase_clusters.append((semtype_nsubj, semtype_dobj, sentence))\n",
    "\n",
    "    return set(verb_structures), set(verb_clusters), set(phrase_clusters)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stampa della collezione"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "per ogni struttura stampa nsubj e dobj\n",
    "'''\n",
    "def print_structures(structures):\n",
    "    i = 0\n",
    "    for nsubj, syn_nsubj, dobj, syn_dobj in structures:\n",
    "        print('Struttura ' + str(i))\n",
    "        print('\\t soggetto: ' + nsubj + ' - ' + str(syn_nsubj))\n",
    "        print('\\t oggetto: ' + dobj + ' - ' + str(syn_dobj))\n",
    "        i += 1\n",
    "        print('\\n')\n",
    "\n",
    "'''\n",
    "per ogni struttura stampa il suo cluster semantico\n",
    "'''\n",
    "def print_clusters(clusters):\n",
    "    i = 0\n",
    "    for nsubj_semtype, dobj_semtype in clusters:\n",
    "        print('Cluster ' + str(i))\n",
    "        print('\\t SemType soggetto: ' + nsubj_semtype)\n",
    "        print('\\t SemType oggetto: ' + dobj_semtype)\n",
    "        i += 1\n",
    "        print('\\n')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = pd.read_csv('english_wikipedia_sentence.csv')\n",
    "structures, clusters, phrase_clusters = generate(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Struttura 0\n",
      "\t soggetto: BC - Synset('bc.r.01')\n",
      "\t oggetto: appearance - Synset('appearance.n.05')\n",
      "\n",
      "\n",
      "Struttura 1\n",
      "\t soggetto: crops - Synset('crop.n.03')\n",
      "\t oggetto: subsidies - Synset('subsidy.n.01')\n",
      "\n",
      "\n",
      "Struttura 2\n",
      "\t soggetto: Crescent - Synset('crescent.n.01')\n",
      "\t oggetto: domestication - Synset('domestication.n.03')\n",
      "\n",
      "\n",
      "Struttura 3\n",
      "\t soggetto: I - Synset('iodine.n.01')\n",
      "\t oggetto: face - Synset('face.n.01')\n",
      "\n",
      "\n",
      "Struttura 4\n",
      "\t soggetto: one - Synset('one.n.01')\n",
      "\t oggetto: projection - Synset('projection.n.02')\n",
      "\n",
      "\n",
      "Struttura 5\n",
      "\t soggetto: workers - Synset('worker.n.01')\n",
      "\t oggetto: success - Synset('success.n.01')\n",
      "\n",
      "\n",
      "Struttura 6\n",
      "\t soggetto: Lord - Synset('lord.v.01')\n",
      "\t oggetto: number - Synset('number.n.01')\n",
      "\n",
      "\n",
      "Struttura 7\n",
      "\t soggetto: philosophy - Synset('philosophy.n.03')\n",
      "\t oggetto: struggle - Synset('struggle.n.01')\n",
      "\n",
      "\n",
      "Struttura 8\n",
      "\t soggetto: Boys - Synset('male_child.n.01')\n",
      "\t oggetto: children - Synset('child.n.01')\n",
      "\n",
      "\n",
      "Struttura 9\n",
      "\t soggetto: Ages - Synset('age.n.01')\n",
      "\t oggetto: improvements - Synset('improvement.n.02')\n",
      "\n",
      "\n",
      "Struttura 10\n",
      "\t soggetto: distortions - Synset('distorted_shape.n.01')\n",
      "\t oggetto: decrease - Synset('decrease.n.01')\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_structures(structures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 0\n",
      "\t SemType soggetto: attribute\n",
      "\t SemType oggetto: act\n",
      "\n",
      "\n",
      "Cluster 1\n",
      "\t SemType soggetto: quantity\n",
      "\t SemType oggetto: communication\n",
      "\n",
      "\n",
      "Cluster 2\n",
      "\t SemType soggetto: all\n",
      "\t SemType oggetto: act\n",
      "\n",
      "\n",
      "Cluster 3\n",
      "\t SemType soggetto: cognition\n",
      "\t SemType oggetto: act\n",
      "\n",
      "\n",
      "Cluster 4\n",
      "\t SemType soggetto: shape\n",
      "\t SemType oggetto: act\n",
      "\n",
      "\n",
      "Cluster 5\n",
      "\t SemType soggetto: substance\n",
      "\t SemType oggetto: body\n",
      "\n",
      "\n",
      "Cluster 6\n",
      "\t SemType soggetto: shape\n",
      "\t SemType oggetto: event\n",
      "\n",
      "\n",
      "Cluster 7\n",
      "\t SemType soggetto: person\n",
      "\t SemType oggetto: event\n",
      "\n",
      "\n",
      "Cluster 8\n",
      "\t SemType soggetto: person\n",
      "\t SemType oggetto: person\n",
      "\n",
      "\n",
      "Cluster 9\n",
      "\t SemType soggetto: group\n",
      "\t SemType oggetto: possession\n",
      "\n",
      "\n",
      "Cluster 10\n",
      "\t SemType soggetto: social\n",
      "\t SemType oggetto: attribute\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_clusters(clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('all',\n",
       "  'act',\n",
       "  'The period 2700–2300 BC saw the first appearance of the Sumerian abacus, a table of successive columns which delimited the successive orders of magnitude of their sexagesimal number system. '),\n",
       " ('attribute',\n",
       "  'act',\n",
       "  'The Middle Ages saw significant improvements in the agricultural techniques and technology. '),\n",
       " ('cognition',\n",
       "  'act',\n",
       "  'In essence, the philosophy sees anarchist struggle as a necessary component of feminist struggle and vice-versa. '),\n",
       " ('group',\n",
       "  'possession',\n",
       "  'Despite this progress, certain crops, such as cotton, still see subsidies in developed countries artificially deflating global prices, causing hardship in developing countries with non-subsidized farmers. '),\n",
       " ('person',\n",
       "  'event',\n",
       "  'Many workers and activists saw Bolshevik success as setting an example; Communist parties grew at the expense of anarchism and other socialist movements. '),\n",
       " ('person',\n",
       "  'person',\n",
       "  'His Boys & Girls Club sees 2,000 children throughout the year and boasts a world-class junior tennis team. '),\n",
       " ('quantity',\n",
       "  'communication',\n",
       "  'If one looks down the axis of the C – C bond, one will see the so-called Newman projection. '),\n",
       " ('shape',\n",
       "  'act',\n",
       "  'The Fertile Crescent of Western Asia first saw the domestication of animals, starting the Neolithic Revolution. '),\n",
       " ('shape',\n",
       "  'event',\n",
       "  'Since the 1980s, policy-driven distortions have seen a greater decrease among livestock products than crops during the worldwide reforms in agricultural policy. '),\n",
       " ('social',\n",
       "  'attribute',\n",
       "  'Lord sees a number of post-Aristotelian interpolations in the Politics, for example, but is generally confident that the work has come down to us relatively intact. '),\n",
       " ('substance',\n",
       "  'body',\n",
       "  'I was giving water to the wounded because I saw your face in all of them, replied Bhai Kanhaiya. ')}"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phrase_clusters"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
