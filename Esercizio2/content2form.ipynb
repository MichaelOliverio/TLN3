{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Esercitazione 2\n",
    "\n",
    "Sempre partendo dai dati sulle definizioni, si richiede di provare a costruire un sistema che utilizzi la molteplicità delle definizioni per risalire al termine \"target\" in maniera automatica. Non si richiede di \"indovinare\" ogni termine, ma di avvicinarsi (almeno semanticamente) alla risposta. Provare più soluzioni, includendo meccanismi di filtro delle definizioni (ad es. escludendo quelle meno informative o con caratteristiche particolari), di ricerca nell'albero tassonomico di WordNet (provando a partire da candidati \"genus\", secondo il principio Genus-Differentia), ecc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def normalize(sentence, only_nouns = False):\n",
    "    tokens = nltk.word_tokenize(sentence)\n",
    "    \n",
    "    if only_nouns:\n",
    "        #mantengo solo i sostantivi\n",
    "        tokens = nltk.pos_tag(tokens)\n",
    "        tokens = [word for word in tokens if word[1] in ['NN', 'NNS', 'NNP', 'NNPS']]\n",
    "        tokens = [word[0] for word in tokens]\n",
    "\n",
    "    tokens = [token for token in tokens if token not in string.punctuation] #tolgo la punteggiatura\n",
    "    tokens = [token.lower() for token in tokens] # sostituisco le maiuscole con le minuscole\n",
    "    tokens = [token for token in tokens if token not in stop_words] # rimuovo le stop words\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens] # lemmatizzo\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metodi di supporto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_context(definitions):\n",
    "    context = set()\n",
    "    for definition in definitions:\n",
    "        context = context.union(set(definition))\n",
    "    return context\n",
    "\n",
    "def simplified_lesk(word, context):\n",
    "    best_sense = None\n",
    "    max_overlap = 0\n",
    "\n",
    "    for sense in wn.synsets(word, pos='n'):\n",
    "        signature = set(normalize(sense.definition())).union(set(normalize(' '.join(sense.examples()))))\n",
    "        overlap = len(context.intersection(signature))\n",
    "        if overlap > max_overlap:\n",
    "            max_overlap = overlap\n",
    "            best_sense = sense\n",
    "    \n",
    "    return best_sense\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ottenimento del geneus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Restituisce come geneus il token che compare più volte nelle definizioni\n",
    "'''\n",
    "def get_geneus(definitions, syn = None):\n",
    "    words = []\n",
    "    for definition in definitions:\n",
    "        words += definition\n",
    "\n",
    "    # se ho il synset aggiungo alle parole anche la definizione del synset\n",
    "    if syn is not None:\n",
    "        words += normalize(syn.definition())\n",
    "\n",
    "    return nltk.FreqDist(words).most_common(1)[0][0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approccio 1: recursive simplified lesk\n",
    "\n",
    "Partendo dal geneus, si prendono tutti i suoi synset, e ad ognuno di essi si applica il recursive_simplified_lesk. La caratteristica\n",
    "di questo algoritmo è che non si ferma al primo livello di iponimi del geneus, ma per ognuno di essi scende di un certo numero di livelli (iperparametro da specificare). Così facendo vengono analizzati un numero maggiore di synsets così da avere più probabilità di ottenere il synset corretto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Variante ricorsiva del lesk, che permette di scendere di livello nella\n",
    "gerarchia del geneus\n",
    "'''\n",
    "def recursive_simplified_lesk(level, best_sense, current_syn, max_overlap, context):\n",
    "    if level == 0:\n",
    "        return [best_sense, max_overlap]\n",
    "    else:\n",
    "        for syn in current_syn.hyponyms():\n",
    "            signature = set(normalize(syn.definition())).union(normalize(' ' .join(syn.examples())))\n",
    "            overlap = len(context.intersection(signature))\n",
    "            if overlap >= max_overlap:\n",
    "                max_overlap = overlap\n",
    "                best_sense = syn\n",
    "\n",
    "        return recursive_simplified_lesk(level - 1, best_sense, current_syn, max_overlap, context)\n",
    "\n",
    "'''\n",
    "Metodo per predire il token dato un insieme di definizioni\n",
    "'''\n",
    "def predict_token(definitions, geneus):\n",
    "    best_sense = None\n",
    "    max_overlap = 0\n",
    "\n",
    "    # ottengo il contesto\n",
    "    context = get_context(definitions)\n",
    "\n",
    "    # per ogni synset del geneus cerco il synset con il massimo overlap \n",
    "    # con il contesto, andando ad usare una variante personalizzata\n",
    "    # del simplified lesk\n",
    "    for syn_geneus in wn.synsets(geneus, pos='n'):    \n",
    "        sense, overlap = recursive_simplified_lesk(5, best_sense, syn_geneus, max_overlap, context)\n",
    "        if overlap > max_overlap:\n",
    "            max_overlap = overlap\n",
    "            best_sense = sense\n",
    "\n",
    "    return best_sense"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approccio 2: rimozione definizioni poco informative + recursive simplified lesk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_uninformative_definitions(definitions, syn):\n",
    "    syn_definition = normalize(syn.definition())\n",
    "\n",
    "\n",
    "    defs_overlap = []\n",
    "    # per ogni definizione calcolo l'overlap con la definizione del synset\n",
    "    for definition in definitions:\n",
    "        defs_overlap.append(len(set(normalize(definition)).intersection(set(syn_definition))))\n",
    "\n",
    "    # rimuovo le definizioni che hanno overlap minore\n",
    "    min_overlap = min(defs_overlap)\n",
    "    \n",
    "    new_definitions = []\n",
    "    for i in range(len(definitions)):\n",
    "        if defs_overlap[i] != min_overlap:\n",
    "            new_definitions.append(normalize(definitions[i], only_nouns=True))\n",
    "\n",
    "    return new_definitions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applicazione approccio 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token: door\n",
      "geneus: room\n",
      "predicted token: toilet.n.01\n",
      "\n",
      "\n",
      "token: ladybug\n",
      "geneus: insect\n",
      "predicted token: lepidopterous_insect.n.01\n",
      "\n",
      "\n",
      "token: pain\n",
      "geneus: sensation\n",
      "predicted token: sound.n.02\n",
      "\n",
      "\n",
      "token: blurriness\n",
      "geneus: image\n",
      "predicted token: likeness.n.02\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "corpus = pd.read_csv('definizioni.tsv', sep='\\t', engine='python')\n",
    "\n",
    "corpus['door'] = corpus['door'].apply(normalize, only_nouns=True)\n",
    "corpus['ladybug'] = corpus['ladybug'].apply(normalize, only_nouns=True)\n",
    "corpus['pain'] = corpus['pain'].apply(normalize, only_nouns=True)\n",
    "corpus['blurriness'] = corpus['blurriness'].apply(normalize, only_nouns=True)\n",
    "\n",
    "for token in corpus.columns:\n",
    "    geneus = get_geneus(corpus[token])\n",
    "    predicted_token = predict_token(corpus[token], geneus)\n",
    "\n",
    "    print('token: ' + token)\n",
    "    print(\"geneus: \" + geneus)\n",
    "    print(\"predicted token: \" + predicted_token.name())\n",
    "\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applicazione approccio 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token:  door\n",
      "Geneus:  room\n",
      "predicted token: toilet.n.01\n",
      "\n",
      "\n",
      "Token:  ladybug\n",
      "Geneus:  insect\n",
      "predicted token: lepidopterous_insect.n.01\n",
      "\n",
      "\n",
      "Token:  pain\n",
      "Geneus:  feeling\n",
      "predicted token: pain.n.02\n",
      "\n",
      "\n",
      "Token:  blurriness\n",
      "Geneus:  image\n",
      "predicted token: visual_image.n.01\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "corpus = pd.read_csv('definizioni.tsv', sep='\\t', engine='python')\n",
    "\n",
    "\n",
    "for token in ['door', 'ladybug', 'pain', 'blurriness']:\n",
    "    definitions = []\n",
    "    for definition in corpus[token]:\n",
    "        definitions.append(normalize(definition))\n",
    "\n",
    "    syn = simplified_lesk(token, get_context(definitions))\n",
    "    new_definitions = remove_uninformative_definitions(corpus[token], syn)\n",
    "    geneus = get_geneus(new_definitions, syn)\n",
    "    predicted_token = predict_token(new_definitions, geneus)\n",
    "\n",
    "    print('Token: ', token)\n",
    "    print('Geneus: ', geneus)\n",
    "    print(\"predicted token: \" + predicted_token.name())\n",
    "    print('\\n')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5238573367df39f7286bb46f9ff5f08f63a01a80960060ce41e3c79b190280fa"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
