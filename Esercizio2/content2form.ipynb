{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "def clear_sentence(sentence, lemmatizer = None, stemmer = None):\n",
    "    \"\"\"\n",
    "    Data una frase la pulisce dai caratteri speciali e lettere maiscole\n",
    "    e ed esegue infine la lemmatizzazione o stemmatizzazione, in base ai\n",
    "    parametri specificati. Oltre a ciò viene fatto lo stopword removal, cioè\n",
    "    vengono tolte parole congiunzioni, articoli, ...\n",
    "    Ritorna un array di parole.\n",
    "    \"\"\"\n",
    "    from nltk.corpus import stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    if lemmatizer is None and stemmer is None:\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    tokens = nltk.word_tokenize(sentence)\n",
    "    tokens = [token for token in tokens if token not in string.punctuation] #tolgo la punteggiatura\n",
    "    tokens = [token.lower() for token in tokens] # sostituisco le maiuscole con le minuscole\n",
    "    tokens = [token for token in tokens if token not in stop_words] # rimuovo le stop words\n",
    "    if stemmer is not None:\n",
    "        tokens = [stemmer.stem(token) for token in tokens] # stemmizzo\n",
    "    else:\n",
    "        tokens = [lemmatizer.lemmatize(token) for token in tokens] # lemmatizzo\n",
    "    \n",
    "    return tokens\n",
    "    \n",
    "def get_tokens_frequency(sentences):\n",
    "    \"\"\"\n",
    "    Data una lista di frasi conta le occorrenze di ogni parola e ritorna un dizionario\n",
    "    con la loro frequenza ordinato dalla parola più frequente a quella meno.\n",
    "    \"\"\"\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    stemmer = PorterStemmer() # non usato\n",
    "\n",
    "    sentences_number = len(sentences)\n",
    "    tokens_occurrences = {}\n",
    "    for i in range (1, len(sentences)):\n",
    "        sentence = sentences[i]\n",
    "\n",
    "        if not isinstance(sentence, float):\n",
    "            tokens = clear_sentence(sentence, lemmatizer, None)\n",
    "            #tokens = clear_sentence(sentence, None, stemmer)\n",
    "\n",
    "            for token in tokens:\n",
    "                if token in tokens_occurrences:\n",
    "                    tokens_occurrences[token] += 1\n",
    "                else:\n",
    "                    tokens_occurrences[token] = 1\n",
    "    \n",
    "    tokens_frequency = {}\n",
    "\n",
    "    for token in tokens_occurrences:\n",
    "        tokens_frequency[token] = tokens_occurrences[token] / sentences_number\n",
    "\n",
    "    # riordino le frequenze in ordine decrescente\n",
    "    tokens_frequency = {k: v for k, v in sorted(tokens_frequency.items(), key=lambda item: item[1], reverse=True)}\n",
    "\n",
    "    return tokens_frequency\n",
    "\n",
    "def get_hypoyms(token):\n",
    "    \"\"\"\n",
    "    Dato un token ritorna una lista di iponimi\n",
    "    \"\"\"\n",
    "    synonyms = wordnet.synsets(token)\n",
    "\n",
    "    hyponyms = []\n",
    "    for syn in synonyms:\n",
    "        hyponyms += syn.hyponyms()\n",
    "\n",
    "    hyponyms = [syn.lemmas()[0].name() for syn in hyponyms]\n",
    "\n",
    "    return hyponyms\n",
    "\n",
    "def get_hypernyms(token):\n",
    "    \"\"\"\n",
    "    Dato un token ritorna una lista di iperonimi\n",
    "    \"\"\"\n",
    "    synonyms = wordnet.synsets(token)\n",
    "\n",
    "    hypernyms = []\n",
    "    for syn in synonyms:\n",
    "        hypernyms += syn.hypernyms()\n",
    "\n",
    "    hypernyms = [syn.lemmas()[0].name() for syn in hypernyms]\n",
    "\n",
    "    return hypernyms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'used': 0.75, 'object': 0.5, 'material': 0.5, 'construction': 0.5, 'build': 0.40625, 'building': 0.3125, 'made': 0.1875, 'clay': 0.1875, 'block': 0.1875, 'something': 0.15625, 'usually': 0.125, 'brick': 0.09375, 'house': 0.09375, 'constructing': 0.0625, 'element': 0.0625, 'like': 0.0625, 'piece': 0.0625, 'shape': 0.0625, 'wall': 0.0625, 'rectangular-shaped': 0.0625, 'e.g': 0.03125, 'aim': 0.03125, '’': 0.03125, 'basic': 0.03125, 'parallelepiped': 0.03125, 'tool': 0.03125, 'resistnat': 0.03125, 'polygonal': 0.03125, 'different': 0.03125, 'size': 0.03125, 'red': 0.03125, 'e.i': 0.03125, 'generally': 0.03125, 'cunstruction': 0.03125, 'e': 0.03125, 'phyisical': 0.03125, 'physical': 0.03125, 'well': 0.03125, 'eg': 0.03125, 'costruction': 0.03125, 'part': 0.03125, 'thing': 0.03125, 'structure': 0.03125, 'rectangular': 0.03125, 'ceramic': 0.03125}\n"
     ]
    }
   ],
   "source": [
    "corpus = pd.read_csv('definizioni.csv', sep=',', engine='python')\n",
    "\n",
    "#emotion\n",
    "emotion_definitions = corpus.iloc[3]\n",
    "emotion_definitions = emotion_definitions.dropna()\n",
    "\n",
    "emotion_tokens_frequency = get_tokens_frequency(emotion_definitions)\n",
    "\n",
    "print(emotion_tokens_frequency)\n",
    "emotion_tokens_frequency = list(emotion_tokens_frequency.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['body_type', 'dumpiness', 'lankiness', 'adult_body', 'female_body', 'juvenile_body', 'male_body', 'person', 'cantilever', 'channelize', 'corduroy', 'customize', 'dry-wall', 'frame', 'groin', 'lock', 'raise', 'rebuild', 'revet', 'wattle']\n"
     ]
    }
   ],
   "source": [
    "hyponyms1 = get_hypoyms('build')\n",
    "print(hyponyms1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['address',\n",
       " 'avail',\n",
       " 'cannibalize',\n",
       " 'enjoy',\n",
       " 'exert',\n",
       " 'exploit',\n",
       " 'exploit',\n",
       " 'fall_back',\n",
       " 'give',\n",
       " 'implement',\n",
       " 'misapply',\n",
       " 'overuse',\n",
       " 'play',\n",
       " 'play',\n",
       " 'ply',\n",
       " 'pull_out_all_the_stops',\n",
       " 'put',\n",
       " 'recycle',\n",
       " 'share',\n",
       " 'strain',\n",
       " 'take',\n",
       " 'waste',\n",
       " 'work',\n",
       " 'board',\n",
       " 'drink',\n",
       " 'abuse',\n",
       " 'pervert',\n",
       " 'spare',\n",
       " 'take',\n",
       " 'waste',\n",
       " 'trespass',\n",
       " 'follow',\n",
       " 'catch',\n",
       " 'charm',\n",
       " 'commemorative',\n",
       " 'curio',\n",
       " 'discard',\n",
       " 'draw',\n",
       " 'film',\n",
       " 'finding',\n",
       " 'floater',\n",
       " 'fomite',\n",
       " 'geological_formation',\n",
       " 'growth',\n",
       " 'hail',\n",
       " 'head',\n",
       " 'hoodoo',\n",
       " 'ice',\n",
       " 'je_ne_sais_quoi',\n",
       " 'keepsake',\n",
       " 'land',\n",
       " 'land',\n",
       " 'location',\n",
       " 'makeweight',\n",
       " 'moon',\n",
       " 'neighbor',\n",
       " 'paring',\n",
       " 'part',\n",
       " 'property',\n",
       " 'remains',\n",
       " 'ribbon',\n",
       " 'shiner',\n",
       " 'snake',\n",
       " 'stuff',\n",
       " 'triviality',\n",
       " 'vagabond',\n",
       " 'wall',\n",
       " 'web',\n",
       " 'whole',\n",
       " 'business',\n",
       " 'grail',\n",
       " 'point',\n",
       " 'thing',\n",
       " 'direct_object',\n",
       " 'indirect_object',\n",
       " 'prepositional_object',\n",
       " 'retained_object',\n",
       " 'antipathy',\n",
       " 'bugbear',\n",
       " 'center',\n",
       " 'execration',\n",
       " 'hallucination',\n",
       " 'infatuation',\n",
       " 'love',\n",
       " 'cavil',\n",
       " 'challenge',\n",
       " 'demur',\n",
       " 'mind',\n",
       " 'raise_hell',\n",
       " 'remonstrate',\n",
       " 'abrasive',\n",
       " 'adhesive_material',\n",
       " 'aggregate',\n",
       " 'ammunition',\n",
       " 'animal_material',\n",
       " 'atom',\n",
       " 'ballast',\n",
       " 'bedding_material',\n",
       " 'bimetal',\n",
       " 'builder',\n",
       " 'chemical',\n",
       " 'coloring_material',\n",
       " 'composite_material',\n",
       " 'conductor',\n",
       " 'contaminant',\n",
       " 'detritus',\n",
       " 'diamagnet',\n",
       " 'discharge',\n",
       " 'dust',\n",
       " 'earth',\n",
       " 'elastomer',\n",
       " 'fiber',\n",
       " 'filling',\n",
       " 'floccule',\n",
       " 'fluff',\n",
       " 'foam',\n",
       " 'HAZMAT',\n",
       " 'homogenate',\n",
       " 'humate',\n",
       " 'impregnation',\n",
       " 'insulator',\n",
       " 'mineral',\n",
       " 'packing_material',\n",
       " 'paper',\n",
       " 'particulate',\n",
       " 'plant_material',\n",
       " 'precursor',\n",
       " 'radioactive_material',\n",
       " 'raw_material',\n",
       " 'rind',\n",
       " 'rock',\n",
       " 'sealing_material',\n",
       " 'sorbate',\n",
       " 'sorbent',\n",
       " 'thickening',\n",
       " 'toner',\n",
       " 'transparent_substance',\n",
       " 'undercut',\n",
       " 'vernix',\n",
       " 'wad',\n",
       " 'waste',\n",
       " 'copy',\n",
       " 'rehash',\n",
       " 'aba',\n",
       " 'acrylic',\n",
       " 'Aertex',\n",
       " 'alpaca',\n",
       " 'baize',\n",
       " 'basket_weave',\n",
       " 'batik',\n",
       " 'batiste',\n",
       " 'belting',\n",
       " 'bombazine',\n",
       " 'boucle',\n",
       " 'broadcloth',\n",
       " 'broadcloth',\n",
       " 'brocade',\n",
       " 'buckram',\n",
       " 'bunting',\n",
       " 'calico',\n",
       " 'cambric',\n",
       " \"camel's_hair\",\n",
       " 'camlet',\n",
       " 'camouflage',\n",
       " 'canopy',\n",
       " 'canvas',\n",
       " 'cashmere',\n",
       " 'cerecloth',\n",
       " 'challis',\n",
       " 'chambray',\n",
       " 'chenille',\n",
       " 'chiffon',\n",
       " 'chino',\n",
       " 'chintz',\n",
       " 'coating',\n",
       " 'cobweb',\n",
       " 'cord',\n",
       " 'cotton',\n",
       " 'cotton_flannel',\n",
       " 'crepe',\n",
       " 'cretonne',\n",
       " 'crinoline',\n",
       " 'damask',\n",
       " 'denim',\n",
       " 'diamante',\n",
       " 'diaper',\n",
       " 'dimity',\n",
       " 'doeskin',\n",
       " 'drapery',\n",
       " 'duck',\n",
       " 'duffel',\n",
       " 'elastic',\n",
       " 'etamine',\n",
       " 'faille',\n",
       " 'felt',\n",
       " 'fiber',\n",
       " 'flannel',\n",
       " 'flannelette',\n",
       " 'fleece',\n",
       " 'foulard',\n",
       " 'frieze',\n",
       " 'fustian',\n",
       " 'gabardine',\n",
       " 'georgette',\n",
       " 'gingham',\n",
       " 'grogram',\n",
       " 'grosgrain',\n",
       " 'haircloth',\n",
       " 'herringbone',\n",
       " 'homespun',\n",
       " 'hopsacking',\n",
       " 'horsehair',\n",
       " 'jaconet',\n",
       " 'jacquard',\n",
       " 'khadi',\n",
       " 'khaki',\n",
       " 'knit',\n",
       " 'lace',\n",
       " 'lame',\n",
       " 'leatherette',\n",
       " 'linen',\n",
       " 'linsey-woolsey',\n",
       " 'lint',\n",
       " 'lisle',\n",
       " 'mackinaw',\n",
       " 'mackintosh',\n",
       " 'madras',\n",
       " 'marseille',\n",
       " 'metallic',\n",
       " 'mohair',\n",
       " 'moire',\n",
       " 'moleskin',\n",
       " \"monk's_cloth\",\n",
       " 'moquette',\n",
       " 'moreen',\n",
       " 'motley',\n",
       " 'mousseline_de_sole',\n",
       " 'muslin',\n",
       " 'nankeen',\n",
       " 'net',\n",
       " 'ninon',\n",
       " 'nylon',\n",
       " 'oilcloth',\n",
       " 'olive_drab',\n",
       " 'organza',\n",
       " 'paisley',\n",
       " 'panting',\n",
       " 'pepper-and-salt',\n",
       " 'percale',\n",
       " 'permanent_press',\n",
       " 'piece_of_cloth',\n",
       " 'pilot_cloth',\n",
       " 'pina_cloth',\n",
       " 'pinstripe',\n",
       " 'pique',\n",
       " 'plush',\n",
       " 'polyester',\n",
       " 'pongee',\n",
       " 'poplin',\n",
       " 'print',\n",
       " 'quilting',\n",
       " 'rayon',\n",
       " 'rep',\n",
       " 'sackcloth',\n",
       " 'sacking',\n",
       " 'sailcloth',\n",
       " 'samite',\n",
       " 'sateen',\n",
       " 'satin',\n",
       " 'satinet',\n",
       " 'screening',\n",
       " 'scrim',\n",
       " 'seersucker',\n",
       " 'serge',\n",
       " 'shag',\n",
       " 'shantung',\n",
       " 'sharkskin',\n",
       " 'sheeting',\n",
       " 'shirting',\n",
       " 'shirttail',\n",
       " 'silesia',\n",
       " 'silk',\n",
       " 'spandex',\n",
       " 'sponge_cloth',\n",
       " 'stammel',\n",
       " 'suede_cloth',\n",
       " 'suiting',\n",
       " \"swan's_down\",\n",
       " 'taffeta',\n",
       " 'tammy',\n",
       " 'tapa',\n",
       " 'tapestry',\n",
       " 'tartan',\n",
       " 'terry',\n",
       " 'ticking',\n",
       " 'toweling',\n",
       " 'tweed',\n",
       " 'twill',\n",
       " 'upholstery_material',\n",
       " 'Velcro',\n",
       " 'velour',\n",
       " 'velvet',\n",
       " 'velveteen',\n",
       " 'vicuna',\n",
       " 'Viyella',\n",
       " 'voile',\n",
       " 'wash-and-wear',\n",
       " 'waterproof',\n",
       " 'web',\n",
       " 'webbing',\n",
       " 'whipcord',\n",
       " 'wincey',\n",
       " 'wire_cloth',\n",
       " 'wool',\n",
       " 'worsted',\n",
       " 'yoke',\n",
       " 'packaging',\n",
       " 'railing',\n",
       " 'roofing',\n",
       " 'crenelation',\n",
       " 'dry_walling',\n",
       " 'erecting',\n",
       " 'fabrication',\n",
       " 'grading',\n",
       " 'house-raising',\n",
       " 'road_construction',\n",
       " 'rustication',\n",
       " 'shipbuilding',\n",
       " 'adjunct',\n",
       " 'clause',\n",
       " 'complement',\n",
       " 'involution',\n",
       " 'phrase',\n",
       " 'predicator',\n",
       " 'crystallization',\n",
       " 'gestation',\n",
       " 'airdock',\n",
       " 'altar',\n",
       " 'arcade',\n",
       " 'arch',\n",
       " 'area',\n",
       " 'balance',\n",
       " 'balcony',\n",
       " 'balcony',\n",
       " 'bascule',\n",
       " 'boarding',\n",
       " 'body',\n",
       " 'bridge',\n",
       " 'building',\n",
       " 'building_complex',\n",
       " 'catchment',\n",
       " 'coil',\n",
       " 'colonnade',\n",
       " 'column',\n",
       " 'corner',\n",
       " 'cross',\n",
       " 'deathtrap',\n",
       " 'defensive_structure',\n",
       " 'door',\n",
       " 'entablature',\n",
       " 'erection',\n",
       " 'establishment',\n",
       " 'false_bottom',\n",
       " 'floor',\n",
       " 'fountain',\n",
       " 'guide',\n",
       " 'honeycomb',\n",
       " 'house_of_cards',\n",
       " 'housing',\n",
       " 'hull',\n",
       " 'jungle_gym',\n",
       " 'lamination',\n",
       " 'landing',\n",
       " 'lookout',\n",
       " 'masonry',\n",
       " 'memorial',\n",
       " 'mound',\n",
       " 'obstruction',\n",
       " 'partition',\n",
       " 'platform',\n",
       " 'porch',\n",
       " 'post_and_lintel',\n",
       " 'prefab',\n",
       " 'projection',\n",
       " 'public_works',\n",
       " 'sail',\n",
       " 'set-back',\n",
       " 'shelter',\n",
       " 'shoebox',\n",
       " 'signboard',\n",
       " 'stadium',\n",
       " 'superstructure',\n",
       " 'supporting_structure',\n",
       " 'tower',\n",
       " 'transept',\n",
       " 'trestlework',\n",
       " 'vaulting',\n",
       " 'ways',\n",
       " 'wellhead',\n",
       " 'wind_tunnel',\n",
       " 'quadrature',\n",
       " 'jerry-building']"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hyponyms = []\n",
    "for i in range(0, 4): #len(emotion_tokens_frequency) - 1):\n",
    "    token = emotion_tokens_frequency[i]\n",
    "    #pos = nltk.pos_tag([token])[0][1][0].lower()\n",
    "\n",
    "    #print('\\ntoken: ', token)\n",
    "    #print('pos: ', pos, '\\n')\n",
    "\n",
    "    hyponyms += get_hypoyms(token)\n",
    "    #hyponyms = [syn.lemmas()[0].name() for syn in hyponyms if syn.pos() == pos]\n",
    "\n",
    "hyponyms\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5238573367df39f7286bb46f9ff5f08f63a01a80960060ce41e3c79b190280fa"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
